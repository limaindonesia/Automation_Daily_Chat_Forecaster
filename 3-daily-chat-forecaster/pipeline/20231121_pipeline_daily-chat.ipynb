{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7aa40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#model\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "from google.cloud import storage\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "03916e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#model\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "from google.cloud import storage\n",
    "import mlflow.sklearn\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4ecdd",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9afcda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_gcs(bucket_name, folder, filename, delimiter=','):\n",
    "    start_time = time.time()  # Start measuring time\n",
    "    blob = storage_client.get_bucket(bucket_name).blob(f'{folder}/{filename}')\n",
    "    csv_data = blob.download_as_text()\n",
    "    df = pd.read_csv(StringIO(csv_data), delimiter=delimiter)\n",
    "    elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "    print(f\"Read {filename} complete. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "def remove_lawyers(df, lawyer_ids):\n",
    "    filtered_df = df[~df['lawyer_id'].isin(lawyer_ids)]\n",
    "    return filtered_df\n",
    "\n",
    "# Function to create lagged features for time series data\n",
    "def create_lagged_features(data, lag):\n",
    "    lagged_data = data.copy()\n",
    "    for i in range(1, lag + 1):\n",
    "        lagged_data[f'Lag_{i}'] = data['count'].shift(i)\n",
    "    return lagged_data\n",
    "\n",
    "def data_process_ml(df, train_index, test_index):\n",
    "    \n",
    "    df = df.set_index('date', inplace=False)\n",
    "\n",
    "    lag = 7  # Number of lagged values, adjust as needed\n",
    "    # Apply the function to create lagged features\n",
    "    lagged_df = create_lagged_features(df, lag)\n",
    "    \n",
    "    X = lagged_df.drop(['count'], axis=1)\n",
    "    y = lagged_df['count']\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def visualize(df, x, y):\n",
    "    # Set a Seaborn style and color palette\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"tab10\")\n",
    "\n",
    "    # Create a line plot\n",
    "    plt.figure(figsize=(12, 6))  # Adjust the figure size as needed\n",
    "    sns.lineplot(data=df, x=x, y=y)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Daily Chat Consultations', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=16)\n",
    "    plt.ylabel('Count', fontsize=16)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    # Add grid lines\n",
    "    plt.grid(True, alpha=0.5)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()  # Ensure all elements fit nicely\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def data_process_stats(df, train_index, test_index):\n",
    "    time_series = df['count']\n",
    "    train_data, test_data = time_series[train_index], time_series[test_index]\n",
    "    return train_data, test_data\n",
    "\n",
    "def model_auto_arima(train_data, test_data):\n",
    "    auto_model = auto_arima(train_data, seasonal=True, m=7, trace=True)\n",
    "    n_forecast = len(test_data)\n",
    "    y_pred, conf_int = auto_model.predict(n_forecast, return_conf_int=True)\n",
    "    return y_pred\n",
    "\n",
    "def data_process_prophet(df, train_index, test_index):\n",
    "    df = df.rename(columns={'date': 'ds', 'count': 'y'}, inplace=False)\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "    return train_data, test_data\n",
    "\n",
    "def model_prophet(train_data_pr, test_data_pr):\n",
    "    model = Prophet()\n",
    "    model.fit(train_data_pr)\n",
    "    y_pred = model.predict(test_data_pr)\n",
    "    return y_pred['yhat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92894ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path\n",
    "relative_path = '../../deep-flash-sa.json'\n",
    "file_path = os.path.abspath(relative_path)\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = file_path\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f58c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read consultations.csv complete. Elapsed time: 3.66 seconds\n"
     ]
    }
   ],
   "source": [
    "df_consultations = read_data_from_gcs('perqara-dendrobium', 'raw/postgres/csv/consultations', 'consultations.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc87e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    #get data consultation\n",
    "    data_consultation = get_data_consultation()\n",
    "    #get data availibity\n",
    "    data_availability = get_data_availability()\n",
    "    #get data web_visitor\n",
    "    data_web = get_data_web_visitor()\n",
    "    #transform consultation\n",
    "    \n",
    "    #transform availability\n",
    "    \n",
    "    #transform web visitor\n",
    "        \n",
    "    #merge\n",
    "    \n",
    "    #load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ac1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_consultation():\n",
    "    data = read_data_from_gcs('perqara-dendrobium', 'raw/postgres/csv/consultations', 'consultations.csv', delimiter='|')\n",
    "    return data\n",
    "\n",
    "def get_data_availability():\n",
    "    data = df_availability_instants = read_data_from_gcs('perqara-dendrobium', 'raw/postgres/csv/availability_instants', 'availability_instants.csv', delimiter=',')\n",
    "    return data\n",
    "    \n",
    "def get_data_web_visitor():\n",
    "    data = pd.read_csv('website-visitor_20231114.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacf6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lawyer(df, lawyer_ids):\n",
    "    filtered_df = df[~df['lawyer_id'].isin(lawyer_ids)]\n",
    "    return filtered_df\n",
    "\n",
    "def filter_status(df):\n",
    "    filtered_df = df[df['status'] == 600]\n",
    "    return filtered_df\n",
    "\n",
    "def cast_column(df, columns):\n",
    "    data = df.copy()\n",
    "    for column in columns:\n",
    "        data[column] = pd.to_datetime(data[column])\n",
    "    return data\n",
    "\n",
    "def count_daily_consultations(df):\n",
    "    count = df['created_at'].to_frame().reset_index(drop=True)\n",
    "    count.set_index('created_at', inplace=True)\n",
    "    df_daily_count = count.resample('D').size().reset_index()\n",
    "    return df_daily_count\n",
    "\n",
    "def rename_columns_consultation(df):\n",
    "    df.rename(columns={'created_at': 'date', 0: 'count'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to create lagged features for time series data\n",
    "def create_lagged_features(data, lag):\n",
    "    lagged_data = data.copy()\n",
    "    for i in range(1, lag + 1):\n",
    "        lagged_data[f'lag_{i}'] = data['count'].shift(i)\n",
    "    data = lagged_data.fillna(lagged_data.mean(numeric_only=True))\n",
    "    return data\n",
    "\n",
    "def filter_columns(df, column):\n",
    "    data = df[column]\n",
    "    return data\n",
    "\n",
    "def calculate_duration(df):\n",
    "    df['duration'] = (df['end_datetime'] - df['start_datetime']).astype('timedelta64[m]') / 60\n",
    "    return df\n",
    "\n",
    "def extract_date(df, column):\n",
    "    df['date'] = df[column].dt.date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "def filter_working_hours(df):\n",
    "    working_hours_mask = (df['start_datetime'].dt.hour >= 10) & (df['end_datetime'].dt.hour <= 18)\n",
    "    return df[working_hours_mask]\n",
    "\n",
    "def filter_late_hours(df):\n",
    "    late_hours_mask = (df['end_datetime'].dt.hour > 18) & (df['end_datetime'].dt.hour <= 24)\n",
    "    return df[late_hours_mask]\n",
    "\n",
    "def get_daily_lawyer_count(df, time_column, count_column):\n",
    "    return df.groupby(df[time_column].dt.date)[count_column].nunique()\n",
    "\n",
    "def fill_date_range(daily_count):\n",
    "    date_range = pd.date_range(start=daily_count.index.min(), end=daily_count.index.max(), freq='D')\n",
    "    data = daily_count.reindex(date_range, fill_value=0).fillna(0)\n",
    "    data = data.reset_index()\n",
    "    return data\n",
    "\n",
    "def rename_index(df):\n",
    "    df = df.rename(columns={'index': 'date'})\n",
    "    return df\n",
    "\n",
    "def create_df_lawyer_count(working_hours_lawyers, late_hours_lawyers):\n",
    "    new_data = {'working_hours_lawyers': working_hours_lawyers, 'late_hours_lawyers': late_hours_lawyers}\n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "def rename_columns_website(df):\n",
    "    column_mapping = {'event_date': 'date', 'f0_': 'web_visitor'}\n",
    "    return df.rename(columns=column_mapping)\n",
    "\n",
    "def convert_date_to_datetime(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "    return df\n",
    "\n",
    "def generate_date_range(start_date, end_date):\n",
    "    return pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "def calculate_mean_count(df, count_column):\n",
    "    return df[count_column].mean()\n",
    "\n",
    "def create_new_data(date_range, mean_count):\n",
    "    new_data = {'date': date_range, 'web_visitor': mean_count}\n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "def concatenate_dataframes(df1, df2):\n",
    "    return pd.concat([df1, df2]).sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e147cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_consultation(df):\n",
    "    # cleaning\n",
    "    df = remove_lawyers(df, [36, 38, 48, 120, 192, 195])\n",
    "    # filter status 6000\n",
    "    df = filter_status(df)\n",
    "    # cast columns\n",
    "    df = cast_column(df, ['created_at'])\n",
    "    # count consultations\n",
    "    df = count_daily_consultations(df)\n",
    "    # rename column\n",
    "    df = rename_columns_consultation(df)\n",
    "    # create lagged feature\n",
    "    df = create_lagged_features(df, 7)\n",
    "    return df\n",
    "    \n",
    "def transform_availability(df):\n",
    "    # cleaning\n",
    "    df = remove_lawyers(df, [36, 38, 48, 120, 192, 195])\n",
    "    # filter column\n",
    "    df = filter_columns(df, ['lawyer_id', 'start_datetime', 'end_datetime', 'created_at'])\n",
    "    # cast columns\n",
    "    df = cast_column(df, ['start_datetime', 'end_datetime', 'created_at'])\n",
    "    # calculate duration\n",
    "    df = calculate_duration(df)\n",
    "    # extract date\n",
    "    df = extract_date(df, 'created_at')\n",
    "    # filter working hours\n",
    "    df_working_hours = filter_working_hours(df)\n",
    "    # filter late hours\n",
    "    df_late_hours = filter_late_hours(df)\n",
    "    # get daily lawyer count for working hours\n",
    "    working_hours_lawyers = get_daily_lawyer_count(df_working_hours, 'start_datetime', 'lawyer_id')\n",
    "    # get daily lawyer count for late hours\n",
    "    late_hours_lawyers = get_daily_lawyer_count(df_late_hours, 'start_datetime', 'lawyer_id')\n",
    "    # create daily count DataFrame\n",
    "    df_new = create_df_lawyer_count(working_hours_lawyers, late_hours_lawyers)\n",
    "    # fill date range\n",
    "    df_new = fill_date_range(df_new)\n",
    "    # rename\n",
    "    df_new = rename_index(df_new)\n",
    "    return df_new\n",
    "    \n",
    "def transform_web_visitor(df):\n",
    "    \n",
    "    df = rename_columns_website(df)\n",
    "    \n",
    "    df = convert_date_to_datetime(df)\n",
    "    \n",
    "    # Generate date range\n",
    "    date_range = generate_date_range('2023-05-06', '2023-06-25')\n",
    "\n",
    "    # Calculate mean count\n",
    "    mean_count = calculate_mean_count(df, 'web_visitor')\n",
    "\n",
    "    # Create new data\n",
    "    new_rows = create_new_data(date_range, mean_count)\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    df = concatenate_dataframes(df, new_rows)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf8f8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_date_range(df, start_date, end_date):\n",
    "    mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "    return df[mask]\n",
    "\n",
    "def merge_dataframes(df1, df2, df3):\n",
    "    # Merge the DataFrames on the 'date' column\n",
    "    merged_df = pd.merge(df1, df2, on='date', how='outer')\n",
    "    merged_df = pd.merge(merged_df, df3, on='date', how='outer')\n",
    "    \n",
    "    # Sort the DataFrame by 'date'\n",
    "    merged_df = merged_df.sort_values('date')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57dfd701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read consultations.csv complete. Elapsed time: 3.48 seconds\n",
      "Read availability_instants.csv complete. Elapsed time: 1.91 seconds\n"
     ]
    }
   ],
   "source": [
    "data_consultation = get_data_consultation()\n",
    "data_availability = get_data_availability()\n",
    "data_web = get_data_web_visitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c3de528",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_consultation_tr = transform_consultation(data_consultation)\n",
    "data_availability_tr = transform_availability(data_availability)\n",
    "data_web_tr = transform_web_visitor(data_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8b68c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consultation_tr = filter_date_range(data_consultation_tr, '2023-05-06', '2023-11-13')\n",
    "data_availability_tr = filter_date_range(data_availability_tr, '2023-05-06', '2023-11-13')\n",
    "data_web_tr = filter_date_range(data_web_tr, '2023-05-06', '2023-11-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf459209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>working_hours_lawyers</th>\n",
       "      <th>late_hours_lawyers</th>\n",
       "      <th>web_visitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-06</td>\n",
       "      <td>1</td>\n",
       "      <td>15.757282</td>\n",
       "      <td>15.55122</td>\n",
       "      <td>15.627451</td>\n",
       "      <td>15.704433</td>\n",
       "      <td>15.613861</td>\n",
       "      <td>15.452736</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>533.652482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.55122</td>\n",
       "      <td>15.627451</td>\n",
       "      <td>15.704433</td>\n",
       "      <td>15.613861</td>\n",
       "      <td>15.452736</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533.652482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>15.627451</td>\n",
       "      <td>15.704433</td>\n",
       "      <td>15.613861</td>\n",
       "      <td>15.452736</td>\n",
       "      <td>15.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>533.652482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.704433</td>\n",
       "      <td>15.613861</td>\n",
       "      <td>15.452736</td>\n",
       "      <td>15.32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533.652482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.613861</td>\n",
       "      <td>15.452736</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533.652482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  count      lag_1     lag_2      lag_3      lag_4      lag_5  \\\n",
       "0 2023-05-06      1  15.757282  15.55122  15.627451  15.704433  15.613861   \n",
       "1 2023-05-07      0   1.000000  15.55122  15.627451  15.704433  15.613861   \n",
       "2 2023-05-08      0   0.000000   1.00000  15.627451  15.704433  15.613861   \n",
       "3 2023-05-09      3   0.000000   0.00000   1.000000  15.704433  15.613861   \n",
       "4 2023-05-10      0   3.000000   0.00000   0.000000   1.000000  15.613861   \n",
       "\n",
       "       lag_6  lag_7  working_hours_lawyers  late_hours_lawyers  web_visitor  \n",
       "0  15.452736  15.32                    0.0                 1.0   533.652482  \n",
       "1  15.452736  15.32                    0.0                 0.0   533.652482  \n",
       "2  15.452736  15.32                    1.0                 1.0   533.652482  \n",
       "3  15.452736  15.32                    2.0                 0.0   533.652482  \n",
       "4  15.452736  15.32                    0.0                 0.0   533.652482  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = merge_dataframes(data_consultation_tr, data_availability_tr, data_web_tr)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efa9922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('feature_ml.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8812066",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0f8553b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('feature_ml.csv')\n",
    "df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "176f02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ml model\n",
    "## -train model linear regression\n",
    "## -train model lightgbm\n",
    "## -train model xgboost\n",
    "\n",
    "# choose ml model\n",
    "\n",
    "\n",
    "# save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f09d0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timesries_data(df, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = tscv.split(df)\n",
    "    return splits\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test):\n",
    "    y_pred = model(X_train, y_train, X_test)\n",
    "    metrics = produce_metrics(y_test, y_pred)\n",
    "    return metrics\n",
    "\n",
    "def model_linear_regression(X_train, y_train, X_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "    \n",
    "def model_lgb(X_train, y_train, X_test):\n",
    "    model = lgb.LGBMRegressor(verbose=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "    \n",
    "def model_xgboost(X_train, y_train, X_test):\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def produce_metrics(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    return [mae, rmse, mape]\n",
    "    \n",
    "def metrics_results(model_score):\n",
    "    num_rows = len(model_score)\n",
    "    # Initialize a list to store the column sums\n",
    "    column_sums = []\n",
    "\n",
    "    # Calculate the sum of each column\n",
    "    for col in range(len(model_score[0])):\n",
    "        col_sum = sum(row[col] for row in model_score)\n",
    "        column_sums.append(col_sum / num_rows)\n",
    "\n",
    "    return column_sums\n",
    "\n",
    "def train_ml(df, model_name):\n",
    "    models = {\n",
    "        'Linear Regression': model_linear_regression,\n",
    "        'LightGBM': model_lgb,\n",
    "        'XGBoost': model_xgboost\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    splits = split_timesries_data(df)\n",
    "    \n",
    "    X = df.drop('count', axis=1)\n",
    "    y = df['count']\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(splits):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    \n",
    "        y_pred = models[model_name](X_train, y_train, X_test)\n",
    "        \n",
    "        metrics = produce_metrics(y_test, y_pred)\n",
    "        scores.append(metrics)\n",
    "    \n",
    "    average_score = metrics_results(scores)\n",
    "    print(f\"Print {model_name} score: {average_score}\")\n",
    "    return average_score\n",
    "\n",
    "def best_model(scores, metric):\n",
    "    index = {'MAE': 0, 'RMSE': 1, 'MAPE': 2}\n",
    "    \n",
    "    if metric not in index:\n",
    "        print(\"Invalid metric\")\n",
    "        return None\n",
    "    \n",
    "    metric_index = index[metric]\n",
    "    min_value = min(score[metric_index] for score in scores)\n",
    "    \n",
    "    best_model_index = [i for i, score in enumerate(scores) if score[metric_index] == min_value][0]\n",
    "    models = ['Linear Regression', 'LightGBM', 'XGBoost']  # Update with your actual model names\n",
    "    best_model_name = models[best_model_index]\n",
    "\n",
    "    print(f\"Best model for {metric}: {best_model_name} with {metric} of {min_value}\")\n",
    "    return best_model_name, min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f54da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(df, best_model_name):\n",
    "    \n",
    "    X = df.drop('count', axis=1)\n",
    "    y = df['count']\n",
    "    \n",
    "    if best_model_name == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "    if best_model_name == \"LightGBM\":\n",
    "        model = lgb.LGBMRegressor(verbose=-1)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "    if best_model_name == \"XGBoost\":\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.fit(X, y)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.sklearn.log_model(model, \"best_model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "093138cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Linear Regression score: [5.908098920368892, 7.608417910844674, 947189531089036.6]\n",
      "Print LightGBM score: [6.7620595311001965, 8.702145804912673, 748121394042056.6]\n",
      "Print XGBoost score: [7.214254293361591, 9.338427526216872, 684549064795751.0]\n",
      "Best model for MAE: Linear Regression with MAE of 5.908098920368892\n"
     ]
    }
   ],
   "source": [
    "#PIPELINE\n",
    "\n",
    "#train lr\n",
    "score_lr = train_ml(df, 'Linear Regression')\n",
    "#train lightgbm\n",
    "score_lgbm = train_ml(df, 'LightGBM')\n",
    "#train xgboost\n",
    "score_xgb = train_ml(df, 'XGBoost')\n",
    "\n",
    "#choose\n",
    "best_model_name, _ = best_model([score_lr, score_lgbm, score_xgb], 'MAE')\n",
    "\n",
    "#train model\n",
    "##hyperparameter tuning\n",
    "\n",
    "#log model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "14f3d529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear Regression'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bcda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14730b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975cb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
